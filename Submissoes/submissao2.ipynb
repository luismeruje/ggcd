{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from IPython.display import Image  \n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countNonNulls(array):\n",
    "    return array.notnull().sum()\n",
    "\n",
    "def padWithZero(array):\n",
    "    return array.asfreq().fillna(0)\n",
    "\n",
    "def data_x(year, data):\n",
    "    year_to_slice = year\n",
    "    data_year_to_slice = data[(data.date > datetime.date(year_to_slice,1,1)) &\n",
    "                 (data.date < datetime.date((year_to_slice+1),1,1)) ]  \n",
    "    return data_year_to_slice\n",
    "\n",
    "def testXgBoost(predictors,target):\n",
    "    dmatrix = xgb.DMatrix(data=predictors,label=target)\n",
    "\n",
    "    predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, target, test_size=0.2, random_state=1234)\n",
    "\n",
    "    model = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.6, learning_rate = 0.1,\n",
    "                    max_depth = 5, alpha = 10, n_estimators = 30)\n",
    "\n",
    "    model.fit(predictors_train,target_train)\n",
    "\n",
    "    predictors_test.isnull().sum()\n",
    "    predictors_test.shape\n",
    "    predictors_test.columns.values\n",
    "\n",
    "    #predictors_test = predictors_test.loc[~(np.isnan(predictors_test['Open']))]   \n",
    "    #target_test = target_test.loc[~(np.isnan(target_test))]   \n",
    "\n",
    "\n",
    "    predictions = model.predict(predictors_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(target_test, predictions))\n",
    "    print(\"RMSE: \",rmse)\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be found at: https://www.kaggle.com/mczielinski/bitcoin-historical-data.  We are going to use the dataset referring to the coinbase exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinbase_data = pd.read_csv('coinbase.csv')\n",
    "print(coinbase_data.head(20),\"\\n\")\n",
    "\n",
    "print(coinbase_data.shape,\"\\n\")\n",
    "\n",
    "print(\"Columns: \", list(coinbase_data.columns.values),\"\\n\")\n",
    "\n",
    "print(coinbase_data.describe(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is composed by 8 columns, with each entry representing a time period of 1 minute:\n",
    "* Timestamp - Unix timestamp, seconds elapsed since January 1, 1970\n",
    "* Open - Price of first transaction of the minute. \n",
    "* High - Highest price registered for a transaction during this time period\n",
    "* Low - Sames as high, but gives the lowest price\n",
    "* Close - Price registered in the last transaction\n",
    "* Volume_(BTC) - Total amount of BTC(bitcoin) traded.\n",
    "* Volume_(Currency) - Total amount of USD traded.\n",
    "* Weighted_Price - Average price of a BTC unit during the time period.\n",
    "\n",
    "Above we can see the first 20 entries of the dataset and it's column names. \n",
    "\n",
    "Since we have null values, we will have to address them later. We can also see that the dataset is composed by a total of 2099760 entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to explore both models that are and aren't aware of time. Since we'll want to run a realtime simulation, for models that aren't aware of time (such as random forests), all features will have to be shifted back by a factor of one, to simulate a real scenario were we would only have access to the OHLC(Open,High,Low,Close) and volume values of the previous time period. We'll also leave this for a later section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing null values\n",
    "\n",
    " When a record as at least one null value, the remaining values are also null, except for the timestamp. We can prove this by removing all the entries where all the values, except timestamp, are null, and confirming that no null values remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of missing values\n",
    "print(coinbase_data.isnull().sum(), \"\\n\\n\")\n",
    "\n",
    "copyData = coinbase_data.copy()\n",
    "\n",
    "print(\"Removing null rows...\")\n",
    "\n",
    "#Remove entries where all features, except timestamp, are null\n",
    "copyData = copyData.loc[~(np.isnan(copyData['Open']) \n",
    "                                        & np.isnan(copyData['High']) \n",
    "                                        & np.isnan(copyData['Low']) \n",
    "                                        & np.isnan(copyData['Close']) \n",
    "                                        & np.isnan(copyData['Volume_(BTC)']) \n",
    "                                        & np.isnan(copyData['Volume_(Currency)']) \n",
    "                                        & np.isnan(copyData['Weighted_Price']))]\n",
    "\n",
    "#any missing values?\n",
    "print(\"Are there any missing values? \",copyData.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the kaggle data description, this null rows represent time periods where either the exchange service was down or there were no trades being fulfilled.\n",
    "\n",
    "Below we show a heatmap of null values and a graph of non null values per day. We can see that the amount of null values diminishes as more days go by. This is probably due to the fact that when the exchange opened the amount of trades was small. Later on, null values probably represent maintenance or failure periods of the coinbase platform, during which users couldn't trade in the exchange.\n",
    "Rows were grouped by day to facilitate data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective copyData\n",
    "copyData = coinbase_data.copy()\n",
    "\n",
    "# Had date\n",
    "copyData['date'] = pd.to_datetime(copyData['Timestamp'],unit='s').dt.date\n",
    "\n",
    "#Heatmap with missing values\n",
    "heatmap = copyData.copy()\n",
    "heatmap.index = heatmap['date']\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "heatmap = heatmap.iloc[:, :-1] \n",
    "sns.heatmap(heatmap.isnull(), cbar=False, ax = ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with count of non null values per day\n",
    "nonNullsPerDay = copyData.groupby('date', as_index=False).agg({\n",
    "        'Open'  : countNonNulls\n",
    "    })\n",
    "print(nonNullsPerDay.head(20))\n",
    "nonNullsPerDay.set_index(pd.DatetimeIndex(nonNullsPerDay['date']), inplace=True)\n",
    "nonNullsPerDay = nonNullsPerDay.drop('date',axis=1)\n",
    "nonNullsPerDay = nonNullsPerDay.resample('D').asfreq().fillna(0)\n",
    "nonNullsPerDay = nonNullsPerDay.rename(index=str, columns={'Open': 'NonNullCount'})\n",
    "print(nonNullsPerDay.head(20))\n",
    "print(nonNullsPerDay.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Non null values per day\n",
    "plt.plot(range(1,len(nonNullsPerDay.index)+1),nonNullsPerDay['NonNullCount'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make a generic exploratory analysis. We see how bitcoin price varied overtime and what are the correlation levels between variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Variation of bitcoin's price\n",
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "copyData[\"Weighted_Price\"].plot(grid = True)\n",
    "plt.title('Price variation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using seaborn\n",
    "f,ax=plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(copyData.corr(), annot=True, linewidths=.5, fmt= '.2f',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012 = copyData[copyData.date < datetime.date(2013,1,1)] #No transactions\n",
    "data_2013 = data_x(2013,copyData) #No transactions\n",
    "data_2014 = data_x(2014,copyData) #27 transactions\n",
    "data_2015 = data_x(2015,copyData) #507435 transactions\n",
    "data_2016 = data_x(2016,copyData) #501586 transactions\n",
    "data_2017 = data_x(2017,copyData) #514769 transactions\n",
    "data_2018 = data_x(2018,copyData) #523827 transactions\n",
    "data_2019 = data_x(2019,copyData) #9966 transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dim_array = [ data_2012.shape[0], data_2013.shape[0], data_2014.shape[0],\n",
    "                 data_2015.shape[0], data_2016.shape[0], data_2017.shape[0],\n",
    "                 data_2018.shape[0], data_2019.shape[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_years = range(2012,2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos = [i for i, _ in enumerate(one_dim_array)]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "plt.bar(x_pos, one_dim_array, color='green')\n",
    "plt.xlabel(\"Years\")\n",
    "plt.ylabel(\"Number of transactions\")\n",
    "plt.title(\"Number of transactions per year\")\n",
    "\n",
    "plt.xticks(x_pos, list(array_years))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model analysis and data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example using a gradient boosting framework, xg_boost, with an underlying tree. This first example only adjusts the values so that an entry only knows previous values. \n",
    "The gradient boosting does not understand which time it refers to in that particular moment that the prediction is being made, so it was necessary to make the shift in the values in order to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinbase_data = pd.read_csv('coinbase.csv')\n",
    "coinbase_data = coinbase_data.loc[~(np.isnan(coinbase_data['Open']) \n",
    "                                        & np.isnan(coinbase_data['High']) \n",
    "                                        & np.isnan(coinbase_data['Low']) \n",
    "                                        & np.isnan(coinbase_data['Close']) \n",
    "                                        & np.isnan(coinbase_data['Volume_(BTC)']) \n",
    "                                        & np.isnan(coinbase_data['Volume_(Currency)']) \n",
    "                                        & np.isnan(coinbase_data['Weighted_Price']))]\n",
    "nonTimeAwareDataset = coinbase_data.copy()\n",
    "print(nonTimeAwareDataset.isnull().values.any())\n",
    "#TODO: upsample removed NAN entries\n",
    "\n",
    "#Can it predict anything with just the previous record values?\n",
    "nonTimeAwareDataset = nonTimeAwareDataset.drop(['Timestamp'],axis=1)\n",
    "nonTimeAwareDataset['Weighted_Price'] = nonTimeAwareDataset['Weighted_Price'].shift(-1)\n",
    "\n",
    "predictors, target = nonTimeAwareDataset.iloc[:,:-1],nonTimeAwareDataset.iloc[:,-1]\n",
    "print(predictors.dtypes)\n",
    "print(target.dtypes)\n",
    "print(predictors.head())\n",
    "print(target.head())\n",
    "model = testXgBoost(predictors,target)\n",
    "#RMSE: 5002. Can't really predict anything, plot doesn't even show predictors\n",
    "xgb.plot_importance(model,max_num_features=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Maybe there is a seasonality \n",
    "#One hot encoding of month and day of month\n",
    "#add year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try adding previous prices, since the previous one didn't give us a usable rmse value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonTimeAwareDataset.insert(loc = 0, column = 'laggedPriceBy1', value = nonTimeAwareDataset['Weighted_Price'].shift(1))\n",
    "nonTimeAwareDataset.insert(loc = 0, column = 'laggedPriceBy2', value = nonTimeAwareDataset['Weighted_Price'].shift(2))\n",
    "nonTimeAwareDataset.insert(loc = 0, column = 'laggedPriceBy3', value = nonTimeAwareDataset['Weighted_Price'].shift(3))\n",
    "\n",
    "predictors, target = nonTimeAwareDataset.iloc[:,:-1],nonTimeAwareDataset.iloc[:,-1]\n",
    "\n",
    "model = testXgBoost(predictors,target)\n",
    "#RMSE: 5002. Can't really predict anything, plot doesn't even show predictors\n",
    "xgb.plot_importance(model,max_num_features=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still don't have an usable model, so we'll try creating a dataframe with only previous prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinbase_data = pd.read_csv('coinbase.csv')\n",
    "coinbase_data = coinbase_data.loc[~(np.isnan(coinbase_data['Open']) \n",
    "                                        & np.isnan(coinbase_data['High']) \n",
    "                                        & np.isnan(coinbase_data['Low']) \n",
    "                                        & np.isnan(coinbase_data['Close']) \n",
    "                                        & np.isnan(coinbase_data['Volume_(BTC)']) \n",
    "                                        & np.isnan(coinbase_data['Volume_(Currency)']) \n",
    "                                        & np.isnan(coinbase_data['Weighted_Price']))]\n",
    "data = {'Weighted_Price': coinbase_data['Weighted_Price'],\n",
    "        'laggedPriceBy1':coinbase_data['Weighted_Price'].shift(1),\n",
    "        'laggedPriceBy2':coinbase_data['Weighted_Price'].shift(2),\n",
    "        'laggedPriceBy3':coinbase_data['Weighted_Price'].shift(3)}\n",
    "nonTimeAwareDataset = pd.DataFrame(data, columns = ['laggedPriceBy1'\n",
    "                                                    ,'laggedPriceBy2'\n",
    "                                                    ,'laggedPriceBy3'\n",
    "                                                    , 'Weighted_Price'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(nonTimeAwareDataset.isnull().values.any())\n",
    "print(nonTimeAwareDataset.head())\n",
    "#TODO: upsample removed NAN entries\n",
    "\n",
    "\n",
    "predictors, target = nonTimeAwareDataset.iloc[:,:-1],nonTimeAwareDataset.iloc[:,-1]\n",
    "print(predictors.dtypes)\n",
    "print(target.dtypes)\n",
    "print(predictors.head())\n",
    "print(target.head())\n",
    "model = testXgBoost(predictors,target)\n",
    "#RMSE: 5002. Can't really predict anything, plot doesn't even show predictors\n",
    "xgb.plot_importance(model,max_num_features=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use the ensemble learning method of random forest to generate a model,\n",
    "in order to predict the weighted_price of the bitcoin in the next\n",
    "period of time.\n",
    "\n",
    "In this first instance we are merely using the package scikit-learn\n",
    "to create a regressor with a random configuration, that we consider to be\n",
    "a good beginning to the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinbase_data = pd.read_csv('coinbase.csv')\n",
    "coinbase_data = coinbase_data.loc[~(np.isnan(coinbase_data['Open']) \n",
    "                                        & np.isnan(coinbase_data['High']) \n",
    "                                        & np.isnan(coinbase_data['Low']) \n",
    "                                        & np.isnan(coinbase_data['Close']) \n",
    "                                        & np.isnan(coinbase_data['Volume_(BTC)']) \n",
    "                                        & np.isnan(coinbase_data['Volume_(Currency)']) \n",
    "                                        & np.isnan(coinbase_data['Weighted_Price']))]\n",
    "nonTimeAwareDataset = coinbase_data.copy()\n",
    "print(nonTimeAwareDataset.isnull().values.any())\n",
    "\n",
    "nonTimeAwareDataset = nonTimeAwareDataset.drop(['Timestamp'],axis=1)\n",
    "nonTimeAwareDataset['Weighted_Price'] = nonTimeAwareDataset['Weighted_Price'].shift(-1)\n",
    "nonTimeAwareDataset.drop(nonTimeAwareDataset.tail(1).index,inplace=True)\n",
    "\n",
    "predictors, target = nonTimeAwareDataset.iloc[:,:-1],nonTimeAwareDataset.iloc[:,-1]\n",
    "\n",
    "print(predictors.head())\n",
    "print(target.head())\n",
    "\n",
    "initial_model_rf = RandomForestRegressor(n_estimators=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, target, test_size=0.2, random_state=1234)\n",
    "\n",
    "initial_model_rf.fit(predictors_train, target_train)\n",
    "\n",
    "# Prediction of the target attribute using the test dataset of predictors.\n",
    "predictions = initial_model_rf.predict(predictors_test)\n",
    "rmse = np.sqrt(mean_squared_error(target_test, predictions))\n",
    "print(\"RMSE: \", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the generated model, we are going to proceed with it's analysis,\n",
    "beginning with the analysis of one of the estimators generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an decision tree from the list of estimators of the model\n",
    "tree = initial_model_rf.estimators_[0]\n",
    "\n",
    "# List with the names of the features\n",
    "features_list = list(predictors_train.columns.values)\n",
    "\n",
    "# Exporting the decision tree, using the format .dot.\n",
    "tree_dot = export_graphviz(tree, max_depth = 3, feature_names = features_list)\n",
    "\n",
    "# Generating a graph of the tree from the .dot data.\n",
    "graph = pydotplus.graphviz.graph_from_dot_data(tree_dot)\n",
    "\n",
    "# Creating and presenting a png image of the graph.\n",
    "Image(graph.create_png())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to improve the model, we will check the importance of the \n",
    "features used, to see witch of them is more important to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = list(initial_model_rf.feature_importances_)\n",
    "features_importance = list(zip(features_list, importances))\n",
    "features_importance = sorted(features_importance, key=lambda a: a[1], reverse=True)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "x_values = list(range(len(importances)))\n",
    "plt.bar(x_values, importances, orientation='vertical')\n",
    "plt.xticks(x_values, features_list, rotation='vertical')\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last method we are using in order to obtain an improve in the performance\n",
    "of the model is the tunning of the hyper-parameters provided by the sklearn\n",
    "random Forest algorithm.\n",
    "\n",
    "The RandomForestRegressor has a total of 12 hyper-parameters.\n",
    "Although all of those can be tuned we are only using the following subset\n",
    "of hyper-parameters:\n",
    "\n",
    "* n_estimators: the number of trees in the forest;\n",
    "* criterion: the function used to measure the quality of a split;\n",
    "* max_depth: the maximum depth of each tree;\n",
    "* min_samples_split: the minimum number of samples required to split an internal node;\n",
    "* min_samples_leaf: the minimum number of samples required to be at a leaf node;\n",
    "* max_features: the number of features to consider when searching for the best split;\n",
    "* bootstrap: a boolean indicating whether bootstrap samples are used when building each tree;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start=10, stop=100, num=10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "min_samples_split = [2, 5, 7, 12]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor()\n",
    "\n",
    "model_rf_random = RandomizedSearchCV(estimator = model_rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "model_rf_random.fit(predictors_train, target_train)\n",
    "\n",
    "print(model_rf_random.best_params_)\n",
    "\n",
    "# Prediction of the target attribute using the test dataset of predictors.\n",
    "predictions = initial_model_rf.predict(predictors_test)\n",
    "rmse = np.sqrt(mean_squared_error(target_test, predictions))\n",
    "print(\"RMSE: \", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
